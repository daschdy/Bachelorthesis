\chapter{Ergebnisse der Literaturrecherche}

\section{Chronologische Entwicklung}

Die Entwicklung didaktischer Simulatoren lässt sich über mehrere Jahrhunderte hinweg nachzeichnen. Erste Vorläufer finden sich bereits 1588, als Agostino Remelli ein sogenanntes \textit{Leserad} konzipierte, das die parallele Nutzung mehrerer Bücher erleichtern sollte und somit als frühe mechanische Lernhilfe gelten kann.\cite{cayetano_geschichte_2022}

Ein bedeutender Meilenstein in der Entwicklung früher Lernmaschinen wurde im Jahr 1866 durch das Patent von Halycon Skinner gesetzt. Er konstruierte ein Gerät, bei dem nach der visuellen Darstellung eines Bildes auf einem Kasten die korrekte Bezeichnung über eine Schreibmaschinentastatur eingegeben werden musste. Die Maschine erwies sich jedoch als äußerst fehleranfällig: Falsche Begriffe wurden bei korrekter Orthographie als richtig gewertet, was die Zuverlässigkeit des Systems stark einschränkte. Im Jahr 1911 griff Herbert Aiken Skinners Konzept auf und entwickelte eine verbesserte Version, die er als „Buchstabiermaschine“ präsentierte. Diese Lernvorrichtung basierte auf einem mechanischen Rahmen, in den mit Buchstaben beschriftete Karten eingesetzt wurden. Die puzzleartige Konstruktion erlaubte das Ausprobieren verschiedener Kombinationen, wobei ausschließlich die korrekte Lösung zum Erfolg führte und das „Puzzle“ vervollständigte. Bis 1936 wurden rund 700 Patente für derartige Übungsmaschinen angemeldet.Das grundsätzliche Lernkonzept hinter diesen Maschinen ist \textit{law of effect} des amerikanischen Psychologen Edward L. Thorndike. \parencite[S. 3]{niegemann_kompendium_2008}

Bereits 1928 stellte Sidney Leavitt Pressey eine "Maschine für Intelligenztests" vor, die zu den einflussreichsten Vorläufern späterer Lehrmaschinen zählt. Sie arbeitete mit Multiple-Choice-Aufgaben, wobei die Lernenden ihre Antworten über nummerierte Tasten eingaben. Ein eingebauter Zähler registrierte die Anzahl richtiger Lösungen. Bemerkenswert ist, dass Pressey darüber hinaus -- in Anlehnung an das zeitgleich entwickelte "law of effect" -- einen Belohnungsmechanismus integrierte: Bei einer korrekten Antwort gab die Maschine eine Süßigkeit aus. Damit machte Pressey deutlich, dass psychologische Faktoren wie Motivation und Verstärkung zentrale Elemente bei der Gestaltung von Lern- und Lehrmaschinen sein können.\parencite[S. 705]{benjamin_history_1988}\parencite[S. 969f]{skinner_teaching_1958} 

In den 1950er Jahren erhielten die von Burrhus F. Skinner und James G. Holland entwickelten Lehrmaschinen zunehmende Aufmerksamkeit. Diese "Skinner-Holland’schen Maschinen" beruhten explizit auf der behavioristischen Lerntheorie und folgten dem Prinzip des programmierten Lernens: Der Stoff wurde in kleine Einheiten (sog. \textit{Frames}) zerlegt, auf die jeweils Fragen folgten. Lernende konnten ihre Eingabe direkt mit der richtigen Lösung vergleichen und erhielten so unmittelbares Feedback.\parencite[S.~970--977]{skinner_teaching_1958} Den verfolgten Ansatz von Skinner und Holland kann als \textit{Reaktionszentrierter Ansatz} bezeichnet werden. Die lineare Abfolge des Lehrmaterials wird durch den lehrenden festgelegt und es erfolgt lediglich eine Rückmeldung, wenn die Antwort korrekt ist.\TODO{Verweis auf diesen Ansatz in Kap. 3}

Im Gegensatz zu den linearen Lehrprogrammen entwickelte Norman Crowder im Jahr 1959 das Konzept der sogenannten „verzweigten Lernprogramme“, die eine erste Form der Individualisierung im Lehr-Lernprozess ermöglichten. Diese Programme reagierten adaptiv auf die Fehler der Lernenden, indem sie unterschiedliche Lernpfade eröffneten. Charakteristisch für Crowders Ansatz sind umfangreichere Frames, die jeweils durch eine Multiple-Choice-Frage abgeschlossen werden. Bei Auswahl einer falschen Antwort erhält der Lernende eine spezifische Rückmeldung, die auf die Art des Fehlers abgestimmt ist. Anschließend erfolgt die Weiterführung des Programms entlang einer fehlerabhängigen Sequenz, wobei gegebenenfalls zuvor behandelte Inhalte erneut präsentiert werden, sofern ein Verständnisdefizit vermutet wird.\parencite[S. 252 - 254]{crowder_differences_1963}\parencite[S. 9]{schonfeld_computerbasiertes_2006}


Ab den 1960er Jahren fand die computergestützte Instruktion zunächst weite Verbreitung, verlor jedoch Mitte des Jahrzehnts an öffentlicher Aufmerksamkeit. Neue Impulse gingen von großangelegten US-Projekten aus, die 1971 durch die National Science Foundation initiiert wurden. Dazu zählten PLATO (Programmed Logic for Automatic Teaching Operation) und TICCIT (Time-shared Interactive Computer Controlled Information Television), die erstmals computerbasierte Instruktion in großem Maßstab demonstrierten.

Parallel dazu wurden in Deutschland seit 1964 verschiedene Lehrmaschinen entwickelt, darunter der Robbimat 0, der Geromat III und der Bakkalaureus, die vor allem für Gruppenschulung eingesetzt wurden. Weitere Forschungsinitiativen entstanden mit dem Bildungstechnologischen Zentrum in Wiesbaden und dem „Forschungs- und Entwicklungszentrum für objektivierte Lehr- und Lernverfahren“ (FEoLL) in Paderborn. In Berlin wurde zudem das Projekt ALCU („Algorithmierung von Lehrprogrammen für computergesteuerten Unterricht“) erprobt.

In den frühen 1970er Jahren entstanden an der Universität Freiburg Programme wie PFLABE, das Biologiestudierenden Wissen als Ersatz für Praktika vermitteln sollte, sowie ZOPRAM, das Vorwissensunterschiede zwischen Studierenden ausgleichen sollte.

Nach einer Phase des Rückgangs in den späten 1970er und frühen 1980er Jahren gewann das computergestützte Lernen Mitte der 1990er Jahre durch die Einführung des Begriffs E-Learning neuen Auftrieb.

Maßgeblich hierfür war die zunehmende Verfügbarkeit von Personal Computern und der Aufschwung des World Wide Webs gegen Ende der 1990er Jahre.

 Mit der Initiative „Schulen ans Netz“ wurde 2002 schließlich die flächendeckende Internetanbindung deutscher Schulen erreicht.

In den 2000er Jahren entstanden zahlreiche vom BMBF geförderte Projekte, die sich explizit mit der Visualisierung und Simulation in der Informatik-Didaktik beschäftigten. Dazu gehörten etwa SIMBA („Schlüsselkonzepte der Informatik in multimedialen Bausteinen“), MuSofT („Multimedia in der Softwaretechnik“), RaVi („Rechnerarchitektur-Visualisierung“) sowie die Wissenswerkstatt Rechensysteme (WWR). Diese Projekte zielten darauf ab, komplexe Vorgänge in Rechnersystemen durch interaktive und multimediale Simulationen für Lernende erfahrbar zu machen.

Heute sind didaktische Simulatoren fester Bestandteil der Hochschullehre, insbesondere in der technischen Informatik. Sie werden genutzt, um abstrakte Prozesse wie den Aufbau von Rechnerarchitekturen, Befehlssatzverarbeitung oder Betriebssystemabläufe interaktiv und visuell nachvollziehbar zu machen. Dabei haben sich Trends wie Web 2.0, Serious Games und zunehmend auch KI-basierte adaptive Systeme herausgebildet, die den individuellen Lernfortschritt stärker berücksichtigen und exploratives Lernen fördern

\section{Überblick anhand von Themen}

\section{Überblick anhand von Simulatortypen}
