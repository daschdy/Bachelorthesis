\section{Chronologische Entwicklung der Rechnerarchitektur}\label{chap:3-3-development_ca}

Seit der Veröffentlichung der ersten General-Purpose-Computer in den 1940er Jahren hat sich die Computertechnologie in rasantem Tempo weiterentwickelt. Insbesondere die vergangenen fünf Jahrzehnte waren von einer Steigerung der Rechenleistung, Speicherfähigkeit und Energieeffizienz geprägt \parencite[S.~2]{hennessy_computer_2011}.

\sh{Die Mikroprozessor-Ära} 
(1970er -- 1980er): Ein entscheidender Wendepunkt in der Entwicklung war die Einführung des \textit{Mikroprozessors} Ende der 1970er Jahre. Mikroprozessoren integrierten erstmals alle zentralen Funktionseinheiten auf einem einzigen Chip \parencite[S.~9]{dumas_ii_computer_2006}. Dies führte zu erheblichen Kostensenkungen und ermöglichte eine jährliche Leistungssteigerung von etwa 30--40~\%. Durch die Massenproduktion von Mikroprozessoren wurde Computertechnologie in breiten Anwendungsfeldern verfügbar, wodurch die Grundlage für die heutige IT-Industrie geschaffen wurde \parencites[S.~2]{hennessy_computer_2011}[S.~11]{dumas_ii_computer_2006}.

In den 1970er Jahren dominierten zunächst \textit{\ac{CISC}} die eine Vielzahl komplexer Maschinenbefehle bereitstellten (z.B. IBM System/360, Intel 8086, Motorola 68000). Ziel war es, die Programmierung zu vereinfachen und und den knappen Speicher effizienter zu nutzen, indem einzelne Instruktionen komplexe Operationen abbilden konnten \parencite[S.~12]{dumas_ii_computer_2006}.

Gleichzeitig gewannen höhere Programmiersprachen wie C an Bedeutung, wodurch die Abhängigkeit von Assembler-Code abnahm. Mit dem Aufkommen von UNIX und später Linux standen zudem portierbare Betriebssysteme zur Verfügung, die die Kosten neuer Architekturen reduzierten und die Entwicklung komplexer Software-Ökosysteme förderten \parencites[S.~2]{hennessy_computer_2011}[S.~12]{dumas_ii_computer_2006}.

\sh{\acs{RISC}-Revolution und Instruction-Level Parallelism}
(1980er -- 1990er): Mit der Weiterentwicklung von Compilern sowie dem fallenden Preis von Hauptspeicher wurde jedoch deutlich, dass diese Komplexität die Pipeline-Fähigkeit einschränkte und eine Steigerung der Taktfrequenzen erschwerte. Als Reaktion darauf entstand Anfang der 1980er Jahre die \textit{\ac{RISC}-Bewegung}, die auf einen reduzierten, regelmäßigen Befehlssatz setzte und damit eine deutlich effizientere Ausnutzung von \textit{\ac{ILP}}\footnote{\acf{ILP} bezeichnet die Fähigkeit einer Prozessorarchitektur mehrere Maschinenbefehle gleichzeitig oder überlappend auszuführen, anstatt diese strikt nacheinander abzuarbeiten. Bekannte ILP-Techniken sind z.B. \textit{Pipelining}, \textit{Superskalarität}, \textit{Out-Of-Order Execution} oder \textit{Branch-Prediction} \cite{rau_instruction-level_2011}.} ermöglichte \parencite[S.~2]{hennessy_computer_2011}.  

Parallel dazu entwickelten sich Cache-Speicher zu einem zentralen Bestandteil moderner Prozessoren. Beginnend mit einfachen First-Level-Caches wurde die Speicherhierarchie schrittweise um weitere Ebenen (L1, L2, L3) erweitert, um die zunehmende Lücke zwischen Prozessor- und Hauptspeichergeschwindigkeit zu überbrücken \parencite[S.~2]{hennessy_computer_2011}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.90\textwidth]{img/Cache-Hierarchie.png}
    \caption{Klassische Drei-Level Cache Hierarchie}~\cite{gao_cspm_2022}
    \label{fig:cache_hierarchie}
\end{figure}

\sh{Von Single-Core zum Multi-Core}
(2000er): Mit der Jahrtausendwende stießen Single-Core-Architekturen an physikalische Grenzen: Höhere Taktfrequenzen führten zu überproportionalem Energieverbrauch und Wärmeentwicklung\footnote{Basierend auf dem \textit{Moore’s Law} (Verdopplung der Transistoranzahl auf gleicher Fläche etwa alle zwei Jahre) beschreibt \textit{Dennard Scaling}, dass mit der Verkleinerung von Transistoren sowohl die Taktfrequenz als auch die Transistoranzahl gesteigert werden können, während die Leistung konstant bleibt. Mit weiter fortschreitender Miniaturisierung konnte die Versorgungsspannung jedoch nicht mehr proportional reduziert werden, wodurch Leckströme zunahmen und das Gesetz schließlich zusammenbrach \cite{hennessy_new_2019}.}. Dies markierte das Ende der \enquote{freien} Leistungssteigerungen durch höhere Frequenzen. Die Antwort der Industrie war die Orientierung zu Parallelismus: Mehrkernprozessoren, \ac{SMT} und spezialisierte Hardwareeinheiten ermöglichten weiterhin Leistungszuwächse, allerdings unter deutlich komplexeren Bedingungen für die Softwareentwicklung \parencites[S.~3f]{shalf_new_2007}[S.~67f]{parkhurst_single_2006}.

Parallel dazu nahm die Bedeutung mobiler Endgeräte wie Smartphones und Tablets zu. Mikroprozessoren bildeten hier die Basis, während Fortschritte in der Halbleiterfertigung die hohe Transistordichte und Energieeffizienz dieser Geräte ermöglichten \parencite[S.~2]{hennessy_computer_2011}.

\sh{Rechenzentren, Cloud und neue Software-Paradigmen}
(2010er): Die 2010er Jahre waren geprägt von der Ausbreitung großskaliger Rechenzentren und \textit{Warehouse-Scale Computers}. Diese Systeme setzen nicht mehr auf einzelne Hochleistungsprozessoren, sondern auf die Verwaltung tausender standardisierter Mikroprozessoren in Clustern ausgelegt \parencites[S.~6]{hennessy_computer_2011}[S.~158]{kanev_profiling_2015}[S.~29]{mars_heterogeneity_2011}.

Gleichzeitig veränderte sich die Softwarelandschaft: Produktivität rückte stärker in den Vordergrund, wodurch Sprachen wie Java oder Python populärer wurden, auch wenn dies auf Kosten der maximalen Performance ging \parencite[S.~2]{hennessy_computer_2011}. Mit dem Aufkommen von \textit{\ac{SaaS}} verschoben sich zentrale Rechenprozesse in die Cloud, was neue Anforderungen an Architektur und Skalierbarkeit stellte \parencites[S.~158]{kanev_profiling_2015}[S.~29]{mars_heterogeneity_2011}.

\sh{Aktuelle Entwicklungen: \acs{RISC}, GPUs, \acs{KI}}
(2020er): In den letzten Jahren zeigt sich zunehmend, dass das Potenzial der \ac{ILP} weitgehend ausgeschöpft ist. Der Schwerpunkt in der Forschung verschiebt sich daher auf neue Formen der Parallelisierung \parencite[S.~10]{hennessy_computer_2019}. So wird \ac{DLP} vor allem durch Vektorprozessoren und \ac{SIMD}-Instruktionen realisiert \parencite[S.~1]{israel_high_2024}, während \ac{TLP} weiterhin durch Multi-Core- und Many-Core-Architekturen eine zentrale Rolle spielt \parencite[S.~10]{hennessy_computer_2019}. Darüber hinaus gewinnen \ac{DSA} wie GPUs oder \acp{TPU} an Bedeutung, da sie auf spezifische Anwendungsfelder zugeschnitten sind – insbesondere auf das Training und die Inferenz neuronaler Netze \parencite[S.~10]{hennessy_computer_2019}.

Durch die weltweite Verbreitung von ARM-basierten Systemen in Smartphones, Tablets und \acs{IoT}-Geräten sowie durch den Einstieg von Apple mit den ARM-basierten \enquote{Silicon}-Chips (M1 bis M4) erlebt das \ac{RISC}-Paradigma eine erneute Relevanz. ARM, als eine führende \ac{RISC}-Architektur, etabliert sich zunehmend nicht nur im mobilen Bereich, sondern auch in Rechenzentren, da energieeffiziente Designs immer stärker an Bedeutung gewinnen \parencites[S.~817]{rahman_redefining_2024}[S.~1]{hubner_apple_2025} .

Zudem zeigt eine systematische Untersuchung von \acs{SoC}-Implementierungen, dass ARM- und RISC-V-Designs hinsichtlich Leistungsfähigkeit und Energieeffizienz vergleichbar sind und ARM in vielen Szenarien führend ist \parencite[S.~12771]{suarez_comprehensive_2024}. Wang et. al~\cite{wang_fann--mcu_2020} verdeutlichen, dass für Aufgaben im \acs{IoT}-Bereich hybride Plattformen mit RISC-V und ARM konkurrenzfähig sind und Vorteile beim Energieverbrauch sowie bei der Anpassungsfähigkeit bieten. 

GPUs, ursprünglich für grafikbezogene Berechnungen konzipiert, haben sich zu hocheffizienten Plattformen für parallele Datenverarbeitung entwickelt und bilden heute das Fundament des \textit{Deep Learning}. Ergänzend treiben spezialisierte KI-Beschleuniger\footnote{\enquote{Ein Beschleuniger für künstliche Intelligenz, auch bekannt als KI-Chip, Deep-Learning-Prozessor oder \ac{NPU}, ist ein Hardware-Beschleuniger, der KI-basierte neuronale Netzwerke, Deep Learning und maschinelles Lernen beschleunigt.} vgl.~\cite{ibm_was_2024}.}, etwa von Google, NVIDIA oder Apple, die Weiterentwicklung entscheidend voran \cite{richsrdson_10_2025}. Parallel dazu experimentiert die Forschung mit neuromorphen Architekturen \parencite[S.~22]{schuman_survey_2017} sowie mit Quantencomputern \parencite[S.~2f]{preskill_quantum_2018}, die langfristig völlig neue Rechenparadigmen eröffnen könnten.  

